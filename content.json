{"posts":[{"title":"Hadoop 文件系统","text":"Hadoop 有一个抽象的文件系统概念。Java 抽象类 org.apache.hadoop.fs.FileSystem 定义了 Hadoop 中一个文件系统的客户端接口，并且该抽象类有几个具体实现，其中常用的如下表： 文件系统 URI 方案 Java 实现 描述 Local file:///path fs.LocalFileSystem 使用客户端检验和的本地磁盘文件系统。使用 RawLocalFileSystem 表示无校验和的本地磁盘文件系统。 HDFS hdfs://host/path hdfs.DistributedFileSystem Hadoop 的分布式文件系统 FTP ftp://host/path fs.ftp.FTPFileSystem 由 FTP 服务器支持的文件系统 SFTP sftp://host/path fs.sftp.SFTPFileSystem 由 SFTP 服务器支持的文件系统 其中 Local 文件系统的 URI 方案比较特殊，冒号后有三个斜杠 (///)。这是因为 URL 标准规定 file URL 采用 file://&lt;host&gt;/&lt;path&gt; 形式。作为一个特例，当主机是本机时， 是空字符串。因此，本地 file URL 通常具有三个斜杠。 从 Hadoop URL 读取数据","link":"/blog/2022/09/08/Hadoop%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"title":"Git 学习记录","text":"Git 是目前世界上最先进、最流行的分布式版本控制系统。Git 采用 C 语言开发，并完全免费开源，由 Linus Torvalds 发起，并作为主要开发者。他同时还是 Linux 内核的最早作者，担任 Linux 内核的首要架构师与项目协调者。 配置用户安装完 Git 后一般要配置用户名和邮箱，以便在每次提交中记录下来，方便查找每次提交的用户。Git 的配置一共有三个级别：system（系统级）、global（用户级）、local（版本库）。system 的配置整个系统只有一个，global 的配置每个账号只有一个，local 的配置存在于 Git 版本库中，可以对不同的版本库配置不同的 local 信息。这三个级别的配置是逐层覆盖的关系，当用户提交修改时，首先查找 system 配置，其次查找 global 配置，最后查找 local 配置，逐层查找的过程中，若查询到配置信息，则会覆盖上一层配置，记录在提交记录中。当有多个账号信息时，为了区分不同账户提交的记录。可以配置 global 为常用的用户和邮箱信息。对于不常用的，可以在对应的版本库里配置单独的用户和邮箱信息。 12git config --global user.name &quot;username&quot;git config --global user.email &quot;email address&quot; 创建版本库 新建一个空文件夹，并切换至目录下12mkdir test_gitcd test_git 初始化版本库，此后该目录下的所有文件都将被 Git 管理1git init 添加并提交文件到版本库新建/修改/删除文件的行为都可以被 Git 管理。 对文件进行了以上操作后，将所有的文件变动添加至 Git 暂存区，用于后续提交到版本库1git add . 提交到版本库1git commit -m '提交信息' 版本管理概念解释工作区工作区是指用户新建的可见目录，其下存放着用户自己创建和修改的工作文件。 版本库版本库就是使用 git init 创建出 .git 隐藏目录，称为 Git 版本库。Git 的版本库里存了很多东西，其中最重要的就是称为 stage（或者叫 index）的暂存区，还有 Git 为我们自动创建的第一个分支 master，以及指向 master 的一个指针叫 HEAD。当用户完成文件修改后，使用 git add 命令就可以将文件变动添加至 Git 暂存区，如果用户发现不想添加本次修改，可以使用 git checkout --&lt;file&gt; 撤销指定文件的添加。此时还没有生成新的版本库，如果确认添加无误，使用 git commit 提交本次所有修改，生成新的版本库，并且清空所有暂存区的文件变动。git status 可以时刻观察当前仓库的状态，git log 可以查看每次 commit 的相关信息。提交后，用 git diff HEAD -- &lt;file&gt; 命令可以查看工作区和版本库里面最新版本的区别。 撤销修改把文件在工作区的修改全部撤销，让这个文件回到最近一次 git commit 或 git add 时的状态 1git checkout -- &lt;file&gt; 在 git add后，把暂存区的修改撤销掉（unstage），重新放回工作区 1git reset HEAD &lt;file&gt; 总结： 场景 1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令 git checkout -- &lt;file&gt;。 场景 2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令 git reset HEAD &lt;file&gt;，就回到了场景 1，第二步按场景 1 操作。 删除文件当删除工作区的文件时，工作区和版本库的文件就不一致了，git status 命令会立刻告诉你哪些文件被删除了。现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令 git rm 删掉，并且 git commit： 12git rm &lt;file&gt;git commit -m &quot;remove file&quot; 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： 1git checkout -- &lt;file&gt; git checkout 其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 版本退回退回到上个版本 1git reset --hard HEAD^ 退回到指定版本 1234# 查看版本号git log# 退回到指定版本git reset --hard &lt;commit id&gt; 当找不到目标 commit id 时，Git 提供了一个命令 git reflog 用来记录你的每一次命令： 1git reflog 总结： HEAD 指向的版本就是当前版本，因此，Git 允许我们在版本的历史之间穿梭，使用命令 git reset --hard &lt;commit id&gt;。 穿梭前，用 git log 可以查看提交历史，以便确定要回退到哪个版本。 要重返未来，用 git reflog 查看命令历史，以便确定要回到未来的哪个版本。 远程仓库添加远程库以 GitHub 为例，创建一个空的 GitHub 仓库，然后将此仓库添加至本地远程库： 1git remote add origin git@github.com:imaginefish/blog.git 这里远程库的名字就是 origin，这是 Git 默认的叫法，也可以改成别的。 推送本地库到远程库1git push -u origin main 以上命令会把本地的 main 分支推送到远程库。 由于远程库是空的，我们第一次推送 mian 分支时，加上了-u 参数，Git 不但会把本地的 main 分支内容推送的远程新的 mian 分支，还会把本地的 main 分支和远程的 main 分支关联起来，在以后的推送或者拉取时就可以简化命令，之后推送就可以省略 -u 参数。 查看远程库1git remote -v 删除远程库1git remote rm origin clone 远程库Git 支持 ssh 和 https 等协议，ssh 协议速度快，https 速度慢，并且每次推送都必须输入口令，但是出于安全考虑，有些网络环境下没有开放 ssh 22 端口，则只能使用 https 协议。 1git clone git@github.com:imaginefish/blog.git 分支管理创建与合并分支创建分支： 1git branch main 切换分支： 123git checkout main# 新版命令git switch main 创建并切换分支（替代以上两条命令）： 123git checkout -b main# 新版命令git switch -c main 查看当前分支： 1git branch 合并指定分支到当前分支： 1git merge &lt;name&gt; 删除分支： 1git branch -d main 分支冲突 当被合并分支的修改内容与当前分支不一致时，合并分支会出现分支冲突。 当 Git 无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。 解决冲突就是把 Git 合并失败的文件手动编辑为我们希望的内容，再提交。 用 git log --graph 命令可以看到分支合并图。 Rebase git rebase 操作可以把本地未 push 的分叉提交历史整理成直线。 git rebase 的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。 使用 GitHub 在 GitHub 上，可以自己创建自己的公开和私有仓库 可以任意 Fork 开源仓库 自己拥有 Fork 后的仓库的读写权限 可以推送 pull request 给官方仓库来贡献代码 将本机生成的公钥内容添加至个人的 GitHub 账户 SSH Keys 中，便能实现本地访问个人的 GitHub 仓库，并使用以下命令测试连接是否成功： 1ssh -T git@github.com 搭建 Git 服务器一般在公司内部，还会搭建 Git 服务器，托管公司自己的代码，提升访问速度和安全性，防止代码泄露。 安装 git1sudo apt-get install git 创建 git 用户，用于运行 git 服务1sudo adduser git 创建 ssh 证书登录创建 SSH Key：1ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 之后可以在用户主目录里找到 .ssh 目录，里面有 id_rsa 和 id_rsa.pub 两个文件，这两个就是 SSH Key 的秘钥对，id_rsa 是私钥，不能泄露出去，id_rsa.pub 是公钥，可以放心地告诉任何人。收集所有需要登录的用户的公钥，把所有公钥导入到 /home/git/.ssh/authorized_keys 文件里，一行一个。 初始化 Git 仓库1git init --bare test.git Git 就会创建一个裸仓库，裸仓库没有工作区，因为服务器上的 Git 仓库纯粹是为了共享，所以不让用户直接登录到服务器上去改工作区，并且服务器上的 Git 仓库通常都以 .git 结尾。然后，把 owner 改为 git：1sudo chown -R git:git test.git 禁用 bash 登录出于安全考虑，第二步创建的 git 用户不允许登录 bash，这可以通过编辑 /etc/passwd 文件完成。找到类似下面的一行：1git:x:1001:1001:,,,:/home/git:/bin/bash 改为：1git:x:1001:1001:,,,:/home/git:/usr/bin/git-bash 这样，git 用户可以正常通过 ssh 使用 git，但无法登录 bash，因为我们为 git 用户指定的 git-bash 每次一登录就自动退出。 克隆远程仓库1git clone git@server:/xxx/test.git","link":"/blog/2022/09/06/Git%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"},{"title":"Maven 学习记录","text":"Maven 介绍依赖管理构建流程使用插件模块管理使用 mvnw发布 Artifact","link":"/blog/2022/09/21/Maven%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"},{"title":"PTV Visum 简介","text":"PTV Visum 是世界领先的交通规划软件，更加具体一点，是领先的多模式出行规划和宏观交通模拟软件,专为交通规划人员设计，以增强城市能力： 进行流量分析，预测和基于 GIS 的数据管理 模拟所有道路使用者以及不同交通方式之间的互动 规划公共交通服务 制定先进并面向未来的交通策略和解决方案 PTV Visum 必须提供什么？ 交通数据作为交通规划师，您将所有交通信息都包含在一个软件中，该软件可以全面概述交通状况。PTV Visum 还可以用于预测和评判开发规划的效果。 下一步交通规划对于 PTV Visum，项目任务不分大小。它即可以计划高速公路运营，又可以规划各阶段的公共交通，涉及多个阶段，并在一个交通规划软件中提供您所需的所有信息。 配有传统和高级需求模型PTV Visum 提供了交通规划软件的所有功能，例如传统的四阶段需求模型。除了标准方法外，该软件还提供了独一无二的高级方法，例如基于出行链的需求模型，而无需使用单独的交通规划软件。 制定切实可行的计划借助 PTV Visum，可以针对不同的交通出行方式（例如私人交通、公共交通、共享交通和无人驾驶）评估不同的策略和多种“假设”方案。交通规划软件可根据目标支持决策和计划，提供可靠的结果，同时最大程度地降低利益相关者的投资风险。 灵活性与可扩展我们知道模型和软件需要最终共享给规划师和其他参与方，因此这对我们制定标准很重要。PTV Visum 通过基于 GIS 几何内置数据模型来支持标准化。交通规划软件还可以通过脚本编写以及用户定义的数据层和属性进行扩展。 多层模拟完全集成的模拟和多方案建模可简化您的方法。PTV Visum 是用于宏观模拟的工具，而 PTV Vissim 是用于微观仿真的工具。两种软件均具有相同的方法，可作为层和层之间的桥梁。 PTV Visum 的应用领域作为交通专业人员，您必须在竞争需求之间取得平衡。您所在城市的每个人都希望获得安全便捷的交通。同时，您需要提供负担得起且高效的可靠解决方案。在 40 多年的研发支持下，PTV Visum 不仅经过科学验证，而且为交通规划解决方案提供了广泛的应用。 可靠的交通规划没有强大而可靠的工具，就无法确定多式联运的战略。因此，您可以依靠PTV Visum 全面了解您的交通模型。PTV Visum 创建交通系统的数字副本，因此您可以了解当前存在的问题并发现机遇。 公共交通规划简化PTV Visum 是唯一提供高度展示公共交通系统的专业交通建模软件。从投资新的线路和车站，到时刻表和车队规模的变化，PTV Visum 都能为您提供计划的整体概览，并为您每次都能做出明智的决定。 塑造出行的未来PTV Visum 是您的数字广场。它是唯一模拟汽车、自行车和乘车共享模型的交通规划软件，以检查它们对任何城市的影响。PTV Visum 适应变化并不断创新，以帮助您成为智慧城市。 解决大规模仿真挑战较大规模的模型可以准确捕获交通和交通模拟，但计算成本较高。因此，PTV Visum 使用基于仿真的动态交通分配，将微观和中观的仿真与高效的运行时间结合在一起。不仅易于使用，而且不需要脚本。 传统四阶段模型宏观四阶段预测是传统交通行业的理论基础，Visum 作为宏观分析软件给交通规划师带来了极大的便利，四阶段法虽然涉及大量的数学模型，但他的整体流程其实非常简单。 交通生成计算每个交通小区的产生和吸引交通量（PA 值）。如何计算？工业界常用的就是吸引率法。规范中常常会给到每一类用地的高峰小时到发率，直接拿用地面积乘以这个比率，即可计算出 PA 值。 交通分布各小区之间的出行量（OD）。把 PA 转化为 OD。上一步只是计算出每个小区的到发总量，但不知道这些到与发是从哪里来，到哪里去的。这一个阶段则是明确各个小区间的出行量。如何计算？最典型的就是重力模型。此外还有机会模型、增长率法等。 方式划分阶段二的 OD 是总的出行量，而这些出行量会由不同的交通方式来分担，比如一万的出行量中，有 3000 是小汽车，有 3000 是公交车，等等。我们流量预测往往是算小汽车，因此方式划分实际是从上一步的 OD 中抽取一部分出来。这一部分对应的理论以离散选择模型（Logit 簇模型）为主。 流量分配将各个交通小区之间的不同交通方式的出行分布具体地分配到各条线路上。把从阶段三出来的流量往路网上分配，最终可以得到每条路的流量。如何分？基于阻抗函数构建最优化模型。阻抗函数可选择最短距离、最短时间，等等。最优化目标有用户最优，系统最优，等等。 参考链接： https://www.ptvgroup.com/zhhttps://zhuanlan.zhihu.com/p/181628752","link":"/blog/2022/09/27/PTV-Visum%E7%AE%80%E4%BB%8B/"},{"title":"Pandas DataFrame 之 groupby()","text":"groupby()groupby() 方法定义： 1234567891011DataFrame.groupby( by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True ) -&gt; DataFrameGroupBy 对一个 DataFrame 使用 groupby() 方法聚合后，返回的数据类型为 pandas.core.groupby.generic.DataFrameGroupBy，其实就是分组后的子 DataFrame，处理时将其当成 DataFrame 处理即可，调用 agg() 或 apply() 方法，完成相关的数据统计。可以通过以下方法遍历 DataFrameGroupBy 类型数据： 1234567891011121314df = pd.DataFrame([ {&quot;year&quot;:&quot;2022&quot; ,&quot;month&quot;:&quot;09&quot;, &quot;date&quot;:&quot;2022-09-13&quot;, &quot;min_temp&quot;:23, &quot;max_temp&quot;:26}, {&quot;year&quot;:&quot;2022&quot; ,&quot;month&quot;:&quot;09&quot;, &quot;date&quot;:&quot;2022-09-14&quot;, &quot;min_temp&quot;:22, &quot;max_temp&quot;:27}, {&quot;year&quot;:&quot;2022&quot; ,&quot;month&quot;:&quot;09&quot;, &quot;date&quot;:&quot;2022-09-15&quot;, &quot;min_temp&quot;:23, &quot;max_temp&quot;:29}])df_g=df.groupby(['year','month'])print(type(df_g))for index,data in df_g: print(type(index)) print(index) print(type(data)) print(data) 12345678&lt;class 'pandas.core.groupby.generic.DataFrameGroupBy'&gt;&lt;class 'tuple'&gt;('2022', '09')&lt;class 'pandas.core.frame.DataFrame'&gt; year month date min_temp max_temp0 2022 09 2022-09-13 23 261 2022 09 2022-09-14 22 272 2022 09 2022-09-15 23 29 可以看出分组后的数据类型为 DataFrameGroupBy，其包含了 index 和 data，index 是包含了分组列的 tuple，data 是该组的子 DataFrame。 agg()agg() 方法定义如下： 123456DataFrameGroupBy.aggregate( func=None, *args, engine=None, engine_kwargs=None, **kwargs) -&gt; DataFrame 以下给出几个示例： 选择某一列进行聚合1df.groupby(['year','month']).max_temp.agg(['min','max','mean']) 123 min max meanyear month 2022 09 26 29 27.333333 给不同列分别聚合1df.groupby(['year','month']).agg({'min_temp': ['mean'],'max_temp': ['min','max','mean']}) 1234 min_temp max_temp mean min max meanyear month 2022 09 22.666667 26 29 27.333333 重命名聚合后的列名称1234df.groupby(['year','month']).agg( min_temp_mean=pd.NamedAgg(column='min_temp', aggfunc='mean'), max_temp_mean=pd.NamedAgg(column='max_temp', aggfunc='mean'),) 123 min_temp_mean max_temp_meanyear month 2022 09 22.666667 27.333333 apply()apply 可以按组应用函数 func 并将结果组合在一起，虽然 apply 是一种非常灵活的方法，但它的缺点是使用它可能比使用更具体的方法（如 agg 或 transform）慢很多。 求分组中 max_temp 的最大值与 min_temp 的最小值的差值1df_g.apply(lambda x: x.max_temp.max() - x.min_temp.min()) 123year month2022 09 7dtype: int64 groupby() 后转为 DataFrame DataFrame.reset_index()如果分组后使用 agg() 进行聚合，返回的是 DataFrame，但是索引列是原来的汇聚列，使用 reset_index(inplace=True) 可以将索引转化为列：123df1=df_g.agg('min')df1.reset_index(inplace=True)print(df1) 12 year month date min_temp max_temp0 2022 09 2022-09-13 22 26 Series.to_frame()如果分组后使用 apply() 进行聚合，则返回的是 Series，to_frame() 是将 Series 转化为 DataFrame 的方法，可以将任意 Series 转化为 DataFrame：123sr = df_g.apply(lambda x: x.max_temp.max() - x.min_temp.min())df1 = sr.to_frame('diff').reset_index()print(df1) 12 year month diff0 2022 09 7","link":"/blog/2022/09/13/Pandas-DataFrame%E4%B9%8Bgroupby/"},{"title":"Spark 之 RDD、DF、DS 创建与转换","text":"Resilient Distributed Datasets（RDD）RDD 是 Resilient Distributed Datasets（弹性分布式数据集）的缩写，是 Spark 中一个重要的抽象概念，它表示跨集群节点且被分区的数据集合，可以并行操作。Spark 为 RDD 提供了丰富的操作算子，可以高效处理数据。 创建 RDD有两种创建 RDD 的方式：并行化驱动程序中的现有集合，或引用外部存储系统中的数据集，例如共享文件系统、HDFS、HBase 或任何提供 Hadoop InputFormat 的数据源。 12345678// 创建 SparkContextval conf = new SparkConf().setAppName(appName).setMaster(master)val sc = new SparkContext(conf)// 并行化集合val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data)// 外部文件val distFile = sc.textFile(&quot;data.txt&quot;) Dataset（DS）Dataset 是分布式数据集合。Dataset 是 Spark 1.6 中添加的一个新接口，它提供了 RDD 的优势（强类型化、使用强大 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优势。 DataFrame（DF）DataFrame 其实是 Dataset[Row] 的别名，其中的数据是按照字段组织的，它在概念上等同于关系数据库中的表或 R/Python 中的 data frame。应用程序可以使用 SparkSession 从现有的 RDD、Hive 表或 Spark 数据源创建 DataFrame。 1234567891011121314151617// 创建 SparkSession val spark = SparkSession .builder() .appName(&quot;app name&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation) .enableHiveSupport() .getOrCreate()// text file（行分割文本）val text_df = spark.read.text(&quot;file.txt&quot;)// json fileval json_df = spark.read.json(&quot;file.json&quot;)// csv fileval csv_df = spark.read.csv(&quot;file.csv&quot;)// parquet fileval parquet_df = spark.read.csv(&quot;file.parquet&quot;)// hive tableval hive_table_df = spark.sql(&quot;select * from database_name.table_name&quot;) RDD to DF通过反射推断创建 DataFrame12345val rdd = sc.parallelize(Seq((&quot;Tom&quot;, 13),(&quot;Lily&quot;, 25)))import spark.implicits._val df = rdd.toDF(&quot;name&quot;,&quot;age&quot;) toDF() 方法定义如下： 1def toDF(colNames: String*): DataFrame 用于将强类型数据集合转换为具有重命名列的通用 DataFrame。在从 RDD 转换为具有有意义名称的 DataFrame 时非常方便。 通过 StructType 创建 DataFrame123456789101112131415import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}import org.apache.spark.sql.Rowval rdd = sc.parallelize(Seq((&quot;Tom&quot;, &quot;13&quot;),(&quot;Lily&quot;, &quot;25&quot;)))//创建 schemaval schema = StructType( List( StructField(&quot;name&quot;, StringType, false), StructField(&quot;age&quot;, IntegerType, false) ))//将 rdd 映射到 rdd[row] 上，并将数据格式化为相应的类型val rdd_row = rdd.map(x =&gt; Row(x._1,x._2.toInt))// 创建 dataframeval df = spark.createDataFrame(rdd_row, schema) 通过定义样例类创建 DataFrame123456789val rdd = sc.parallelize(Seq((&quot;Tom&quot;, &quot;13&quot;),(&quot;Lily&quot;, &quot;25&quot;)))//创建样例类case class User(name: String, age: Int)//将 rdd 映射到 rdd[User] 上val rdd_user = rdd.map(x =&gt; User(x._1,x._2.toInt))// 创建 dataframeval df = spark.createDataFrame(rdd_user)// 更简单一点，可以自动推断出 schema 创建 dataframeval df = rdd_user.toDF() RDD to DS通过定义样例类创建 Dataset123456789val rdd = sc.parallelize(Seq((&quot;Tom&quot;, &quot;13&quot;),(&quot;Lily&quot;, &quot;25&quot;)))//创建样例类case class User(name: String, age: Int)//将 rdd 映射到 rdd[User] 上val rdd_user = rdd.map(x =&gt; User(x._1,x._2.toInt))// 创建 dataframeval ds = spark.createDataset(rdd_user)// 更简单一点，可以自动推断出 schema 创建 datasetval ds = rdd_user.toDS() DS/DF to RDD123val df = spark.read.csv(&quot;file.csv&quot;)// 获取 rddval rdd = df.rdd DF to DS123//创建样例类case class User(name: String, age: Int)val ds = DataFrame.map(x=&gt; User(x.getAs(0), x.getAs(1))) DS to DF1val df = DataSet[DataTypeClass].toDF()","link":"/blog/2022/09/08/Spark%E4%B9%8BRDD%E3%80%81DF%E3%80%81DS%E5%88%9B%E5%BB%BA%E4%B8%8E%E8%BD%AC%E6%8D%A2/"},{"title":"Spark 之自定义输出格式写入文件","text":"Spark 常用的保存文件方式 RDD 保存至文本文件1rdd.saveAsTextFile(&quot;path/result&quot;) RDD 以指定 Hadoop 输出格式保持至文件，仅支持 (key,value) 格式的 RDD1rdd.saveHadoopFile(&quot;path/result&quot;,classOf[T],classOf[T],classOf[outputFormat]) DataFrame 以指定格式保持至文件1df.write.mode(&quot;overwrite&quot;).option(&quot;header&quot;,&quot;true&quot;).format(&quot;csv&quot;).save(&quot;path/result&quot;) 以上都简单的，最普遍的保存文件的方式，但有时候是不能够满足我们的需求，使用上述的文件保存方式保存之后，文件名通常是 part-00000 的方式保存在输出文件夹中，并且还包含数据校验和文件 part-00000.crc 和 .SUCCESS 文件，其中 part-00000.crc 用来校验数据的完整性，.SUCCESS 文件用来表示本次输出任务成功完成。 自定义保存文件创建自定义 FileoutputFormat 类继承 MultipleTextOutputFormat 类并复写以下方法： 12345678910111213141516171819202122232425262728293031import org.apache.hadoop.fs.{FileSystem, Path}import org.apache.hadoop.mapred.{FileOutputFormat, JobConf}import org.apache.hadoop.mapred.lib.MultipleTextOutputFormatimport org.apache.hadoop.io.NullWritableclass CustomOutputFormat() extends MultipleTextOutputFormat[Any, Any] { override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = { //这里的key和value指的就是要写入文件的rdd对 key.asInstanceOf[String] + &quot;.csv&quot; } override def generateActualKey(key: Any, value: Any): String = { //输出文件中只保留value 故 key 返回为空 NullWritable.get() } override def checkOutputSpecs(ignored: FileSystem, job: JobConf): Unit = { val outDir: Path = FileOutputFormat.getOutputPath(job) if (outDir != null) { //相同文件名的文件自动覆盖 //避免第二次运行分区数少于第一次,历史数据覆盖失败,直接删除已经存在的目录 try { ignored.delete(outDir, true) } catch { case _: Throwable =&gt; {} } FileOutputFormat.setOutputPath(job, outDir) } }} 将 RDD 映射为 PairRDD1val pair_rdd = rdd.map(x=&gt;(x.split(&quot;,&quot;)(0),x)).partitionBy(new HashPartitioner(50)) 调用 saveAsHadoopFile 输出1pair_rdd.saveAsHadoopFile(output, classOf[String], classOf[String], classOf[CustomOutputFormat])","link":"/blog/2022/09/08/Spark%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E5%86%99%E5%85%A5%E6%96%87%E4%BB%B6/"},{"title":"Spark 项目 jar 包依赖冲突问题总结","text":"Spark 项目 jar 包加载顺序当我们编写的 Spark 项目的依赖较多时，提交运行任务时便很容易出现因为包冲突导致的 java.lang.NoSuchMethodError 报错。原因是当用户提供 Spark 任务运行时，Spark 需要首先加载自身的依赖库（jars），一般位于 $SPARK_HOME/jars 目录下，然后再加载用户提交的 jar 包，当两者存在同样的 jar 但是版本不同时，如果高低版本不能互相兼容，则会报错。 Spark jar 包加载顺序： SystemClassPath: $SPARK_HOME/jars 即 Spark 安装时候提供的依赖包 UserClassPath: Spark-submit --jars 用户提交的依赖包 UserClassPath: Spark-submit app.jar 用户的 Spark 任务 jar 包 spark-submit 提交指定参数解决包冲突既然 Spark 是顺序加载 jar 包，我们可以尝试通过改变其加载顺序解决依赖冲突。以 jackson-core 为例，其在我 Spark 项目中的 Maven 依赖如下： 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.13.2&lt;/version&gt;&lt;/dependency&gt; 但是提交任务的集群安装的是 Spark 2.2.0 版本，该版本的 Spark 依赖 jackson-core 2.6.5 版本，与我项目中的依赖存在版本冲突，在 2.13.2 版本中，存在 com.fasterxml.jackson.core.JsonParser.currentName() 方法，而在 2.6.5 版本中则没有该方法。当采用 Spark 默认加载 jar 顺序的方式，会加载 jackson-core 2.6.5 版本，并出现以下报错信息： 1java.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonParser.currentName()Ljava/lang/String 此时可以通过指定以下参数，优先加载用户提交的依赖 jar 包： 1234567spark-submit \\--master yarn \\--jars /data/jar/jackson-core-2.13.2.jar \\--conf &quot;spark.driver.userClassPathFirst=true&quot; \\--conf &quot;spark.executor.userClassPathFirst=true&quot; \\--class com.xxx.SparkApp \\spark_app.jar 其中相关参数解释如下： --jars 用于提交用户的依赖包，若有多个依赖包，之间用逗号分开 --conf &quot;spark.driver.userClassPathFirst=true&quot; 指定 driver 优先加载用户提交的 jar 包 --conf &quot;spark.executor.userClassPathFirst=true&quot; 指定 executor 优先加载用户提交的 jar 包 但是当项目有很多依赖都与 Spark 本身的依赖存在冲突时，这种方式显然就非常不灵活了，需要指定所有冲突的 jar 包，相当麻烦。并且为了保持集群上 Class Path 的纯净，不影响 Spark 本身的运行，我们一般会将开发完成的 Spark 项目打包成 uber-jar，即包含所有依赖的 jar，直接提交到 Spark 集群运行，不需要依赖外部 jar，此时就可以利用 maven-plugin-shade 制作 shade jar 来解决冲突了。 利用 maven-plugin-shade 制作 shade jar 解决冲突reloaction 重定位 class 文件使用 shade 提供的重定位功能，可以把指定的类移动到一个全新的包中，实现隔离多个项目依赖同一类的不同版本，以解决版本冲突问题。示例如下： 12345678910111213141516171819202122&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.4&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;relocations&gt; &lt;!-- 重定位 com.fasterxml.jackson 至 com.shade.jackson --&gt; &lt;relocation&gt; &lt;pattern&gt;com.fasterxml.jackson&lt;/pattern&gt; &lt;shadedPattern&gt;com.shade.jackson&lt;/shadedPattern&gt; &lt;/relocation&gt; &lt;/relocations&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; &lt;pattern&gt;：原始包名 &lt;shadedPattern&gt;：重命名后的包名 在上述示例中，我们把 com.fasterxml.jackson 包内的所有子包及 class 文件重定位到了 com.shade.jackson 包内。 拆分模块受提交 Spark 项目时发生的依赖冲突问题启发，如果开发的项目本身也存在依赖冲突问题时，显然通过上述的方法就无法解决了，因为没有办法分隔开对同一依赖的调用过程，当类在调用不同版本的依赖时，都会引用重定位后的依赖，此时只存在一个版本，所以依旧会发生依赖冲突问题。 此时就应该将项目拆分为子模块，将依赖不同版本的代码拆分成独立的子模块，各自重定位有冲突的依赖。 如果项目本身就是一个多模块项目，各模块之间有依赖关系，当模块内部存在较多的依赖冲突时，可以为该模块制作一个纯净的子模块，用于重定位所有有冲突的包，然后给该模块引用，如下图所示： 依赖传递当一个项目的某个模块依赖另一个模块时，如果这两个模块同时依赖同一个依赖包的不同版本时，则打包当前模块时，最终打进 jar 的依赖包是当前模块的依赖版本，而不是被依赖的另一个模块的依赖版本。所以，如果需要打包特定版本的依赖包，则需要在 pom.xml 中手动引入指定版本。举例如下： 当前模块的部分依赖： 123456789101112&lt;!-- 模块依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.xxx&lt;/groupId&gt; &lt;artifactId&gt;model-a&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 模块 a （model-a）的部分依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.13.2&lt;/version&gt;&lt;/dependency&gt; spark-core_2.11 的部分依赖： 123456&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 则打包当前模块时，打入最终 jar 包的是 2.6.5 版本的 jackson-databind 依赖，此时如果想打包 2.13.2 版本，则需要在 pom.xml 中手动引入该版本： 12345678910111213141516171819202122&lt;!-- 模块依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.xxx&lt;/groupId&gt; &lt;artifactId&gt;model-a&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- spark 2.2.0 版本中使用的 jackson 版本是2.6.5 --&gt;&lt;!--但是依赖的 model a 中依赖的 jackson 是 2.13.2 版本，所以需要手动引入，解决包冲突--&gt;&lt;!--不然会报错：com.fasterxml.jackson.core.JsonParser.currentName()Ljava/lang/String --&gt;&lt;!--因为 jackson 2.6.5 版本中不包含此方法--&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.13.2&lt;/version&gt;&lt;/dependency&gt; 参考链接：https://www.playpi.org/2019112901.htmlhttps://www.lynsite.cn/20210713/1X0CvPcgvpOzkHSF/","link":"/blog/2022/09/20/Spark%E9%A1%B9%E7%9B%AEjar%E5%8C%85%E4%BE%9D%E8%B5%96%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"},{"title":"Spark 的序列化问题总结","text":"Java 序列化Java 序列化就是指将一个对象转化为二进制的 byte[] 数组，然后以文件的方式进行保存或通过网络传输，等待被反序列化读取出来。序列化常被用于数据存取和通信过程中。 一个 Java 对象要能序列化，必须实现一个特殊的 java.io.Serializable 接口，它的定义如下： 12public interface Serializable {} Serializable 接口没有定义任何方法，它是一个空接口。我们把这样的空接口称为“标记接口”（Marker Interface）。 但实现该接口不保证该对象一定可以序列化，因为序列化必须保证该对象的所有属性可以序列化。 并且 static 和 transient 修饰的变量不会被序列化，这也是解决序列化问题的方法之一，让不能序列化的引用用 static 和 transient 来修饰。（transient 修饰的变量，是不会被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 是0，对象是 null） 此外还可以实现 readObject() 方法和 writeObject() 方法来自定义实现序列化。 序列化12345678910111213141516import java.io.*;import java.util.Arrays;public class Main { public static void main(String[] args) throws IOException { ByteArrayOutputStream buffer = new ByteArrayOutputStream(); try (ObjectOutputStream output = new ObjectOutputStream(buffer)) { // 写入int: output.writeInt(12345); // 写入String: output.writeUTF(&quot;Hello&quot;); // 写入Object: output.writeObject(Double.valueOf(123.456)); } System.out.println(Arrays.toString(buffer.toByteArray())); }} 反序列化12345try (ObjectInputStream input = new ObjectInputStream(...)) { int n = input.readInt(); String s = input.readUTF(); Double d = (Double) input.readObject();} 为了避免因 class 定义变动导致的反序列不兼容，抛出 InvalidClassException 类不匹配异常，Java 的序列化允许 class 定义一个特殊的 serialVersionUID 静态变量，用于标识 Java 类的序列化“版本”，通常可以由 IDE 自动生成。如果增加或修改了字段，可以改变 serialVersionUID 的值，这样就能自动阻止不匹配的 class 版本： 123public class Person implements Serializable { private static final long serialVersionUID = 2709425275741743919L;} Spark 序列化Spark 是分布式执行引擎，其核心抽象是弹性分布式数据集 RDD，其代表了分布在不同节点的数据。Spark 的计算是在 executor 上分布式执行的，故用户开发的对于 RDD 的 map、flatMap、reduceByKey 等 transformation 操作会有如下的执行过程： 代码中的对象在 driver 本地序列化 对象序列化后传输到远程 executor 节点 远程 executor 节点反序列化对象 最终远程节点执行运算 故对象在 transformation 操作中需要序列化后通过网络传输，然后在 executor 节点反序列化执行运算，则要求对象必须可序列化。 如何解决 Spark 项目中的序列化问题Java 对象如果 RDD 保存的是 Java 对象，则要求使用 Java 机制，实现该对象 class 的序列化，即 class 实现 Serializable 接口。对于不可序列化对象，如果本身不需要存储或传输，则可使用 static 或 trarnsient 修饰；如果需要存储传输，则实现 writeObject()/readObject() 使用自定义序列化方法。 Scala 对象对于 scala 开发 Spark 程序，可以定义样例类（case class）来创建对象，实例化后的对象直接可序列化。 此外还需注意哪些操作在 driver，哪些操作在 executor 执行，因为在driver 端（foreachRDD）实例化的对象，很可能不能在 foreach 中运行，因为对象不能从 drive 序列化传递到 executor 端（有些对象有 TCP 链接，一定不可以序列化）。所以这里一般在 foreachPartitions 或 foreach 算子中来实例化对象，这样对象在 executor 端实例化，没有从 driver 传输到 executor 的过程。 参考链接：https://blog.csdn.net/weixin_38653290/article/details/84503295","link":"/blog/2022/09/20/Spark%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"},{"title":"Unicode 字符编码详讲","text":"Unicode 又称为统一码、万国码、单一码，是国际组织制定的旨在容纳全球所有字符的编码方案，包括字符集、编码方案等，它为每种语言中的每个字符设定了统一且唯一的二进制编码，以满足跨语言、跨平台的要求。 Unicode的实现方式不同于编码方式。一个字符的Unicode编码确定。但是在实际传输过程中，由于不同系统平台的设计不一定一致，以及出于节省空间的目的，对Unicode编码的实现方式有所不同。Unicode的实现方式称为Unicode转换格式（Unicode Transformation Format，简称为UTF）。 Unicode 与 UTF-X 的关系：Unicode 是字符集，UTF-32/ UTF-16/ UTF-8 是三种常见的字符编码方案。 Unicode 码点 码点（Code Point）相当于 ASCII 码中的 ASCII 值，它就是 Unicode 字符集中唯一表示某个字符的标识。 码元（Code Unit）代码单元是指讲以实际 Unicode 编码方案将字符存储在计算机中，所占用的代码单元。对于 UTF-8 来说，代码单元是 8 位二进制数，UTF-16 则是 16 位二进制数。 码点的表示形式码点的表示形式为 U+[XX]XXXX，X 代表一个十六进制数，可以有 4-6 位，不足 4 位前补 0，补足 4 位，超过则按实际位数。具体范围是 U+0000 ~ U+10FFFF。理论大小为 10FFFF + 1 = 0x110000，即 17 * 2^16 = 17 * 65536。 平面为了更好分类管理庞大的码点数，Unicode 把每 65536 个码点作为一个平面，总共 17 个平面（Plane）。 17 个平面中的第一个平面即：BMP（Basic Multilingual Plane 基本多语言平面）。也叫 Plane 0，它的码点范围是 U+0000 ~ U+FFFF。这也是我们最常用的平面，日常用到的字符绝大多数都落在这个平面内。UTF-16 只需要用两字节编码此平面内的字符。没包括进来的字符，放在后面的平面，称为辅助平面。 Unicode 编码码点仅仅是一个抽象的概念，是把字符数字化的一个过程，仅仅是一种抽象的编码。整个编码可分为两个过程。首先，将程序中的字符根据字符集中的编号数字化为某个特定的数值，然后根据编号以特定的方式存储到计算机中。 Unicode 编码的两个层面 抽象编码层面把一个字符编码到一个数字。不涉及每个数字用几个字节表示，是用定长还是变长表示等具体细节。 具体编码层面码点到最终编码的转换即 UTF （Unicode Transformation Format：Unicode 转换格式）。这一层是要把抽象编码层面的数字（码点）编码成最终的存储形式（UTF-X）。码点（code）转换成各种编码（encode），涉及到编码过程中定长与变长两种实现方式，定长的话定几个字节；用变长的话有哪几种字节长度，相互间如何区分等等。 UTF-32 就属于定长编码，即永远用 4 字节存储码点，而 UTF-8、UTF-16 就属于变长存储，UTF-8 根据不同的情况使用 1-4 字节，而 UTF-16 使用 2 或 4 字节来存储码点。 注：在第一层，抽象编码层面，字符与数字已经实现一一对应，对数字编码实质就是对字符编码。 BOMBOM（Byte Order Mark），即字节顺序标识。它用来标识使用哪种端法，它常被用来当做标识文件是以 UTF-8、UTF-16 或 UTF-32 编码的标记。 在 Unicode 编码中有一个叫做零宽度非换行空格的字符 ( ZERO WIDTH NO-BREAK SPACE ), 用字符 FEFF 来表示。 对于 UTF-16 ，如果接收到以 FEFF 开头的字节流， 就表明是大端字节序，如果接收到 FFFE， 就表明字节流 是小端字节序 UTF-8 没有字节序问题，上述字符只是用来标识它是 UTF-8 文件，而不是用来说明字节顺序的。”零宽度非换行空格” 字符 的 UTF-8 编码是 EF BB BF, 所以如果接收到以 EF BB BF 开头的字节流，就知道这是UTF-8 文件。 下面的表格列出了不同 UTF 格式的固定文件头： UTF 编码 固定文件头 UTF-8 EF BB BF UTF-16LE FF FE UTF-16BE FE FF UTF-32LE FF FE 00 00 UTF-32BE 00 00 FE FF Java 获取字符串长度123456789101112131415String s1 = &quot;Hello&quot;;String s2 = &quot;😂😂😂😂&quot;;// 获取字符串长度（等于每个字符的 Unicode 码元个数之和）int length1 = s1.length();int length2 = s2.length();// 获取精准的字符串长度（等于每个字符的 Unicode 码点个数之和）int exactLength1 = s1.codePointCount(0, s1.length());int exactLength2 = s2.codePointCount(0, s2.length());// 输出打印System.out.println(&quot;s1: &quot; + s1);System.out.println(&quot;s2: &quot; + s2);System.out.println(&quot;length1: &quot; + length1);System.out.println(&quot;length2: &quot; + length2);System.out.println(&quot;exact length1: &quot; + exactLength1);System.out.println(&quot;exact length2: &quot; + exactLength2); 输出如下： 123456s1: Hellos2: 😂😂😂😂length1: 5length2: 8exact length1: 5exact length2: 4 Java 默认使用 UTF-8 对字符进行编码。当字符串中包含中文或特殊字符时，用 String.length() 方法可能无法精确获得字符串的个数，因为该方法实际返回的是每个字符的 Unicode 码元个数之和（UTF-8 的码元为一个 8 位二进制数），而中文或特殊字符往往占用不止一个码元（Code Unit），所以获得的数值回大于实际字符串长度。使用 String.codePointCount(int beginIndex, int endIndex) 方法则可以获取精准的字符串长度，因为该方法返回指定范围内每个字符的 Unicode 码点个数之和，而一个 Unicode 码点则对应一个实际字符。","link":"/blog/2022/11/14/Unicode%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E8%AF%A6%E8%AE%B2/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/blog/2022/09/02/hello-world/"},{"title":"Windows 下搭建 Spark","text":"版本选择Spark 部署模式分为本地单机（local）和集群模式，本地单机模式常用于本地开发程序与调试。集群模式又分为 Standalone 模式、Yarn 模式、Mesos 模式通过测试发现，以下版本组合报错信息最少 组件 版本 Spark 3.2.2 Hadoop 3.3.1 Scala 2.12.15 JDK 1.8 Spark 依赖库Spark 3.2.2 的依赖库版本如下： 依赖库 版本 Scala 2.12.15 Hadoop 3.3.1 安装步骤 下载安装 JDK，配置 JAVA_HOME 环境变量，将 JAVA_HOME/bin 添加至 Path 环境变量中。 下载安装 Scala，根据具体的操作系统，按照官网推荐的方式安装，无需配置 SCALA_HOME 环境变量。 下载安装 Hadoop，配置 HADOOP_HOME 环境变量，将 HADOOP_HOME/bin 添加至 Path 环境变量中。若在 Windows 上搭建，则还需要根据具体的 Hadoop 版本下载对应的 winutils.exe 和 hadoop.dll 文件，放入 HADOOP_HOME/bin 路径下，以获得在 Windows 上运行 Spark 的支持，避免以下报错信息：1Exception in thread “main” java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z 下载安装 Spark，配置 SPARK_HOME 环境变量，将 SPARK_HOME/bin 添加至 Path 环境变量中。进入 Spark 目录下的 conf 子目录下，根据需要修改 log4j.properties 等配置文件。log4j.properties 常见的配置如下：12345# 在终端输出 WARN 级别的日志，避免输出过多日志，影响查看log4j.rootCategory=WARN, console# 避免 ERROR ShutdownHookManager: Exception while deleting Spark temp dir 报错log4j.logger.org.apache.spark.util.ShutdownHookManager=OFFlog4j.logger.org.apache.spark.SparkEnv=ERROR 安装 PySpark如果上述安装步骤都已完成，就可以开始使用 Java 或 Scala 开发 Spark 程序了。对于 Python 用户，Spark 也提供了语言支持，只需要在 Spark 安装配置完成后，继续安装 PySpark 就可以使用 Python 开发 Spark 程序了： 使用 PyPI 安装1pip install pyspark=3.2.2 使用 Conda 安装1conda install -c conda-forge pyspark=3.2.2 注意： PySpark 的版本需要和 Spark 的版本保持一致，想要了解更多安装详情可以参考官方文档 报错解决 编码错误1UnicodeDecodeError: 'gbk' codec can't decode byte 0x82 in position 120: illegal multibyte sequence 通过添加以下环境变量解决：1PYTHONIOENCODING=utf8 任务运行时报错1org.apache.spark.SparkException: Python worker failed to connect back 通过添加以下环境变量解决：1PYSPARK_PYTHON=python","link":"/blog/2022/09/05/Windows%E4%B8%8B%E6%90%AD%E5%BB%BASpark/"},{"title":"Hexo + Icarus + GitHub Pages 搭建个人博客","text":"先决条件需要先安装以下程序： Node.js (Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本) Git Node.js 版本限制强烈建议永远安装最新版本的 Hexo，以及推荐的 Node.js 版本。 Hexo 版本 最低兼容的 Node.js 版本 6.0+ 12.13.0 5.0+ 10.13.0 4.1 - 4.2 8.10 4.0 8.6 3.3 - 3.9 6.9 3.2 - 3.3 0.12 3.0 - 3.1 0.10 or iojs 0.0.1 - 2.8 0.10 安装 Hexo使用 npm 全局安装 Hexo。 1npm install -g hexo-cli 安装 icarus 指定路径初始化博客目录，并切换至该路径下，以blog路径为例12hexo init blogcd blog 使用 npm 安装 Hexo1npm install hexo-theme-icarus 配置 Hexo 主题1hexo config theme icarus 创建 GitHub 仓库 创建 GitHub 仓库，并开启 Environments，配置 url 配置 ssh，确保可以 ssh 远程访问 GitHub，可以使用以下命令测试连接是否成功：1ssh -T git@github.com 如果出行以下信息，则说明连接成功：1Hi imaginefish! You've successfully authenticated, but GitHub does not provide bash access. 修改配置Hexo 配置Hexo 的配置文件在 blog 目录下，名为 _config.yml 修改语言为中文简体1language: zh-CN 修改时区1timezone: 'Asia/Shanghai' 修改博客网址，如果不配置会出现文件路径引用错误问题，导致 js、css、图片无法加载1url: https://imaginefish.github.io/blog 修改 hexo 部署方式，推送至 Github 仓库的 gh-pages 分支，实现博客部署1234deploy: type: git repository: git@github.com:imaginefish/blog.git branch: gh-pages Icarus 配置Icarus 的配置文件在 blog 目录下，名为 _config.icarus.yml 该主题导航栏无法跟随 Hexo 语言本地化，需要手动修改配置文件123456menu: 主页: / 归档: /archives 分类: /categories 标签: /tags 关于: /about 设置博主邮箱链接123Email: icon: fas fa-envelope url: mailto:imaginefishes@outlook.com 修改 sidebar 配置，使 toc 随文章下拉滚动123sidebar: left: sticky: true 用户访问量统计1busuanzi: true 布局配置文件布局配置文件遵循着与主题配置文件相同的格式和定义。 _config.post.yml 中的配置对所有文章生效，而 _config.page.yml 中的配置对所有自定义页面生效。 这两个文件将覆盖主题配置文件中的配置。例如，可以在 _config.post.yml 中把所有文章变为两栏布局：12345678910111213141516171819widgets: - type: toc position: left index: true collapsed: true depth: 3 - type: recent_posts position: left - type: categories position: left - type: tags position: left order_by: name amount: show_count: true 在 _config.icarus.yml 中把其他页面仍保持三栏布局：123456789101112131415- position: right type: recent_posts- position: right type: categories- position: left type: archives- position: right type: tags order_by: name amount: show_count: true 以下给出个人的完整 _config.icarus.yml 配置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143version: 5.1.0variant: defaultlogo: /img/logo.svghead: favicon: /img/favicon.svg manifest: name: short_name: start_url: theme_color: background_color: display: standalone icons: - src: '' sizes: '' type: structured_data: title: 梦鱼乡 description: 个人博客 url: https://github.com/imaginefish/blog author: 梦鱼 publisher: publisher_logo: image: meta: - '' rss: /atom.xmlnavbar: menu: 主页: / 归档: /archives 分类: /categories 标签: /tags 关于: /about links: GitHub: icon: fab fa-github url: https://github.com/imaginefish/blogfooter: links: Creative Commons: icon: fab fa-creative-commons url: https://creativecommons.org/ Attribution 4.0 International: icon: fab fa-creative-commons-by url: https://creativecommons.org/licenses/by/4.0/ Download on GitHub: icon: fab fa-github url: https://github.com/ppoffice/hexo-theme-icarusarticle: highlight: theme: atom-one-light clipboard: true fold: unfolded readtime: true update_time: true licenses: Creative Commons: icon: fab fa-creative-commons url: https://creativecommons.org/ Attribution: icon: fab fa-creative-commons-by url: https://creativecommons.org/licenses/by/4.0/ Noncommercial: icon: fab fa-creative-commons-nc url: https://creativecommons.org/licenses/by-nc/4.0/search: type: insight index_pages: truesidebar: left: sticky: true right: sticky: falsewidgets: - position: left type: profile author: 梦鱼 author_title: 大数据开发工程师 location: 中国.上海 avatar: /img/avatar.jpg avatar_rounded: true gravatar: follow_link: https://github.com/imaginefishes social_links: Github: icon: fab fa-github url: https://github.com/imaginefishes Email: icon: fas fa-envelope url: mailto:imaginefishes@outlook.com RSS: icon: fas fa-rss url: /atom.xml - position: left type: toc index: true collapsed: true depth: 3 - position: left type: links links: Hexo: https://hexo.io Icarus: https://ppoffice.github.io/hexo-theme-icarus - position: right type: recent_posts - position: right type: categories - position: left type: archives - position: right type: tags order_by: name amount: show_count: trueplugins: animejs: true back_to_top: true busuanzi: true cookie_consent: type: info theme: edgeless static: false position: bottom-left policyLink: https://www.cookiesandyou.com/ gallery: true katex: true mathjax: true outdated_browser: false progressbar: trueproviders: cdn: jsdelivr fontcdn: google iconcdn: fontawesome 其他问题 文章图片的引用路径若要插入本地图片，在博客根目录下找到 source 文件夹，在其下创建 img 子目录，将图片放置于此，通过 /img/xxx.jpg 路径引入。 文章图片如何居中显示该解决方案来源于 GitHub Issues。找到主题的安装路径，找到 article.styl 文件，我的路径为 node_modules\\hexo-theme-icarus\\include\\style\\article.styl，找到如下位置：1234567891011121314&amp;.article .article-meta, .article-tags color: $text-light .article-meta overflow-x: auto margin-bottom: .5rem .article-more @extend .button.is-light .content word-wrap: break-word font-size: $article-font-size 将 article 部分样式添加如下四行代码：12345678910111213141516171819&amp;.article .article-meta, .article-tags color: $text-light .article-meta overflow-x: auto margin-bottom: .5rem .article-more @extend .button.is-light .content word-wrap: break-word font-size: $article-font-size a img margin: auto display: block","link":"/blog/2022/09/02/hexo-icarus-github-pages-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"title":"本地仓库推送到 GitHub 仓库 main 分支","text":"GitHub 目前已经将用户创建的版本库的默认分支从 master 更改为了 main，详情见GitHub Change Blog。而 Git 默认创建的还是 master 分支，所以在本地仓库同步到 GitHub 时，会出现 error: failed to push some refs to 'github.com:imaginefish/blog.git' 报错，需要切换至 main 分区后再进行操作。 初始化版本库1git init 添加远程仓库1git remote add origin git@github.com:imaginefish/blog.git 拉取 GitHub 仓库到本地同步1git pull --rebase origin main 切换本地分支从 master 到 main1git checkout main 添加所有修改到暂存区1git add . 提交修改到版本库1git commit -m 'some messages' 推送到 GitHub 的 main 分支1git push origin main","link":"/blog/2022/09/05/%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E6%8E%A8%E9%80%81%E5%88%B0GitHub%E4%BB%93%E5%BA%93main%E5%88%86%E6%94%AF/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","link":"/blog/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","link":"/blog/tags/HDFS/"},{"name":"Git","slug":"Git","link":"/blog/tags/Git/"},{"name":"Maven","slug":"Maven","link":"/blog/tags/Maven/"},{"name":"Java","slug":"Java","link":"/blog/tags/Java/"},{"name":"Visum","slug":"Visum","link":"/blog/tags/Visum/"},{"name":"交通仿真","slug":"交通仿真","link":"/blog/tags/%E4%BA%A4%E9%80%9A%E4%BB%BF%E7%9C%9F/"},{"name":"Python","slug":"Python","link":"/blog/tags/Python/"},{"name":"Pandas","slug":"Pandas","link":"/blog/tags/Pandas/"},{"name":"Spark","slug":"Spark","link":"/blog/tags/Spark/"},{"name":"序列化","slug":"序列化","link":"/blog/tags/%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"Unicode","slug":"Unicode","link":"/blog/tags/Unicode/"},{"name":"Hexo","slug":"Hexo","link":"/blog/tags/Hexo/"},{"name":"Icarus","slug":"Icarus","link":"/blog/tags/Icarus/"},{"name":"GitHub","slug":"GitHub","link":"/blog/tags/GitHub/"}],"categories":[{"name":"大数据","slug":"大数据","link":"/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"版本控制","slug":"版本控制","link":"/blog/categories/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"},{"name":"项目构建与管理","slug":"项目构建与管理","link":"/blog/categories/%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA%E4%B8%8E%E7%AE%A1%E7%90%86/"},{"name":"交通规划","slug":"交通规划","link":"/blog/categories/%E4%BA%A4%E9%80%9A%E8%A7%84%E5%88%92/"},{"name":"Python 数据处理","slug":"Python-数据处理","link":"/blog/categories/Python-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"字符编码","slug":"字符编码","link":"/blog/categories/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"},{"name":"个人博客","slug":"个人博客","link":"/blog/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"}],"pages":[{"title":"关于我","text":"待更新…","link":"/blog/about/index.html"}]}