{"posts":[{"title":"Hadoop 文件系统","text":"Hadoop 有一个抽象的文件系统概念。Java 抽象类 org.apache.hadoop.fs.FileSystem 定义了 Hadoop 中一个文件系统的客户端接口，并且该抽象类有几个具体实现，其中常用的如下表： 文件系统 URI 方案 Java 实现 描述 Local file:///path fs.LocalFileSystem 使用客户端检验和的本地磁盘文件系统。使用 RawLocalFileSystem 表示无校验和的本地磁盘文件系统。 HDFS hdfs://host/path hdfs.DistributedFileSystem Hadoop 的分布式文件系统 FTP ftp://host/path fs.ftp.FTPFileSystem 由 FTP 服务器支持的文件系统 SFTP sftp://host/path fs.sftp.SFTPFileSystem 由 SFTP 服务器支持的文件系统 其中 Local 文件系统的 URI 方案比较特殊，冒号后有三个斜杠 (///)。这是因为 URL 标准规定 file URL 采用 file://&lt;host&gt;/&lt;path&gt; 形式。作为一个特例，当主机是本机时， 是空字符串。因此，本地 file URL 通常具有三个斜杠。 从 Hadoop URL 读取数据","link":"/blog/2022/09/08/Hadoop%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"title":"Git 学习记录","text":"Git 是目前世界上最先进、最流行的分布式版本控制系统。Git 采用 C 语言开发，并完全免费开源，由 Linus Torvalds 发起，并作为主要开发者。他同时还是 Linux 内核的最早作者，担任 Linux 内核的首要架构师与项目协调者。 配置用户安装完 Git 后一般要配置用户名和邮箱，以便在每次提交中记录下来，方便查找每次提交的用户。Git 的配置一共有三个级别：system（系统级）、global（用户级）、local（版本库）。system 的配置整个系统只有一个，global 的配置每个账号只有一个，local 的配置存在于 Git 版本库中，可以对不同的版本库配置不同的 local 信息。这三个级别的配置是逐层覆盖的关系，当用户提交修改时，首先查找 system 配置，其次查找 global 配置，最后查找 local 配置，逐层查找的过程中，若查询到配置信息，则会覆盖上一层配置，记录在提交记录中。当有多个账号信息时，为了区分不同账户提交的记录。可以配置 global 为常用的用户和邮箱信息。对于不常用的，可以在对应的版本库里配置单独的用户和邮箱信息。 12git config --global user.name &quot;username&quot;git config --global user.email &quot;email address&quot; 创建版本库 新建一个空文件夹，并切换至目录下12mkdir test_gitcd test_git 初始化版本库，此后该目录下的所有文件都将被 Git 管理1git init 添加并提交文件到版本库新建/修改/删除文件的行为都可以被 Git 管理。 对文件进行了以上操作后，将所有的文件变动添加至 Git 暂存区，用于后续提交到版本库1git add . 提交到版本库1git commit -m '提交信息' 版本管理概念解释工作区工作区是指用户新建的可见目录，其下存放着用户自己创建和修改的工作文件。 版本库版本库就是使用 git init 创建出 .git 隐藏目录，称为 Git 版本库。Git 的版本库里存了很多东西，其中最重要的就是称为 stage（或者叫 index）的暂存区，还有 Git 为我们自动创建的第一个分支 master，以及指向 master 的一个指针叫 HEAD。当用户完成文件修改后，使用 git add 命令就可以将文件变动添加至 Git 暂存区，如果用户发现不想添加本次修改，可以使用 git checkout --&lt;file&gt; 撤销指定文件的添加。此时还没有生成新的版本库，如果确认添加无误，使用 git commit 提交本次所有修改，生成新的版本库，并且清空所有暂存区的文件变动。git status 可以时刻观察当前仓库的状态，git log 可以查看每次 commit 的相关信息。提交后，用 git diff HEAD -- &lt;file&gt; 命令可以查看工作区和版本库里面最新版本的区别。 撤销修改把文件在工作区的修改全部撤销，让这个文件回到最近一次 git commit 或 git add 时的状态 1git checkout -- &lt;file&gt; 在 git add后，把暂存区的修改撤销掉（unstage），重新放回工作区 1git reset HEAD &lt;file&gt; 总结： 场景 1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令 git checkout -- &lt;file&gt;。 场景 2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令 git reset HEAD &lt;file&gt;，就回到了场景 1，第二步按场景 1 操作。 删除文件当删除工作区的文件时，工作区和版本库的文件就不一致了，git status 命令会立刻告诉你哪些文件被删除了。现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令 git rm 删掉，并且 git commit： 12git rm &lt;file&gt;git commit -m &quot;remove file&quot; 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： 1git checkout -- &lt;file&gt; git checkout 其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 版本退回退回到上个版本 1git reset --hard HEAD^ 退回到指定版本 1234# 查看版本号git log# 退回到指定版本git reset --hard &lt;commit id&gt; 当找不到目标 commit id 时，Git 提供了一个命令 git reflog 用来记录你的每一次命令： 1git reflog 总结： HEAD 指向的版本就是当前版本，因此，Git 允许我们在版本的历史之间穿梭，使用命令 git reset --hard &lt;commit id&gt;。 穿梭前，用 git log 可以查看提交历史，以便确定要回退到哪个版本。 要重返未来，用 git reflog 查看命令历史，以便确定要回到未来的哪个版本。 远程仓库添加远程库以 GitHub 为例，创建一个空的 GitHub 仓库，然后将此仓库添加至本地远程库： 1git remote add origin git@github.com:imaginefish/blog.git 这里远程库的名字就是 origin，这是 Git 默认的叫法，也可以改成别的。 推送本地库到远程库1git push -u origin main 以上命令会把本地的 main 分支推送到远程库。 由于远程库是空的，我们第一次推送 mian 分支时，加上了-u 参数，Git 不但会把本地的 main 分支内容推送的远程新的 mian 分支，还会把本地的 main 分支和远程的 main 分支关联起来，在以后的推送或者拉取时就可以简化命令，之后推送就可以省略 -u 参数。 查看远程库1git remote -v 删除远程库1git remote rm origin clone 远程库Git 支持 ssh 和 https 等协议，ssh 协议速度快，https 速度慢，并且每次推送都必须输入口令，但是出于安全考虑，有些网络环境下没有开放 ssh 22 端口，则只能使用 https 协议。 1git clone git@github.com:imaginefish/blog.git 分支管理创建与合并分支创建分支： 1git branch main 切换分支： 123git checkout main# 新版命令git switch main 创建并切换分支（替代以上两条命令）： 123git checkout -b main# 新版命令git switch -c main 查看当前分支： 1git branch 合并指定分支到当前分支： 1git merge &lt;name&gt; 删除分支： 1git branch -d main 分支冲突 当被合并分支的修改内容与当前分支不一致时，合并分支会出现分支冲突。 当 Git 无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。 解决冲突就是把 Git 合并失败的文件手动编辑为我们希望的内容，再提交。 用 git log --graph 命令可以看到分支合并图。 Rebase git rebase 操作可以把本地未 push 的分叉提交历史整理成直线。 git rebase 的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。 使用 GitHub 在 GitHub 上，可以自己创建自己的公开和私有仓库 可以任意 Fork 开源仓库 自己拥有 Fork 后的仓库的读写权限 可以推送 pull request 给官方仓库来贡献代码 搭建 Git 服务器一般在公司内部，还会搭建 Git 服务器，托管公司自己的代码，提升访问速度和安全性，防止代码泄露。 安装 git1sudo apt-get install git 创建 git 用户，用于运行 git 服务1sudo adduser git 创建 ssh 证书登录创建 SSH Key：1ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 之后可以在用户主目录里找到 .ssh 目录，里面有 id_rsa 和 id_rsa.pub 两个文件，这两个就是 SSH Key 的秘钥对，id_rsa 是私钥，不能泄露出去，id_rsa.pub 是公钥，可以放心地告诉任何人，可以其添加至个人的 GitHub 账户 SSH Keys 中，便能实现本地访问 GitHub 仓库。收集所有需要登录的用户的公钥，把所有公钥导入到 /home/git/.ssh/authorized_keys 文件里，一行一个。 初始化 Git 仓库1git init --bare test.git Git 就会创建一个裸仓库，裸仓库没有工作区，因为服务器上的 Git 仓库纯粹是为了共享，所以不让用户直接登录到服务器上去改工作区，并且服务器上的 Git 仓库通常都以 .git 结尾。然后，把 owner 改为 git：1sudo chown -R git:git test.git 禁用 bash 登录出于安全考虑，第二步创建的 git 用户不允许登录 bash，这可以通过编辑 /etc/passwd 文件完成。找到类似下面的一行：1git:x:1001:1001:,,,:/home/git:/bin/bash 改为：1git:x:1001:1001:,,,:/home/git:/usr/bin/git-bash 这样，git 用户可以正常通过 ssh 使用 git，但无法登录 bash，因为我们为 git 用户指定的 git-bash 每次一登录就自动退出。 克隆远程仓库1git clone git@server:/xxx/test.git","link":"/blog/2022/09/06/Git%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"},{"title":"Spark 之 RDD、DF、DS 创建与转换","text":"Resilient Distributed Datasets（RDD）RDD 是 Resilient Distributed Datasets（弹性分布式数据集）的缩写，是 Spark 中一个重要的抽象概念，它表示跨集群节点且被分区的数据集合，可以并行操作。Spark 为 RDD 提供了丰富的操作算子，可以高效处理数据。 创建 RDD有两种创建 RDD 的方式：并行化驱动程序中的现有集合，或引用外部存储系统中的数据集，例如共享文件系统、HDFS、HBase 或任何提供 Hadoop InputFormat 的数据源。 12345678// 创建 SparkContextval conf = new SparkConf().setAppName(appName).setMaster(master)val sc = new SparkContext(conf)// 并行化集合val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data)// 外部文件val distFile = sc.textFile(&quot;data.txt&quot;) Dataset（DS）Dataset 是分布式数据集合。Dataset 是 Spark 1.6 中添加的一个新接口，它提供了 RDD 的优势（强类型化、使用强大 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优势。 DataFrame（DF）DataFrame 其实是 Dataset[Row] 的别名，其中的数据是按照字段组织的，它在概念上等同于关系数据库中的表或 R/Python 中的 data frame。应用程序可以使用 SparkSession 从现有的 RDD、Hive 表或 Spark 数据源创建 DataFrame。 1234567891011121314151617// 创建 SparkSession val spark = SparkSession .builder() .appName(&quot;app name&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation) .enableHiveSupport() .getOrCreate()// text file（行分割文本）val text_df = spark.read.text(&quot;file.txt&quot;)// json fileval json_df = spark.read.json(&quot;file.json&quot;)// csv fileval csv_df = spark.read.csv(&quot;file.csv&quot;)// parquet fileval parquet_df = spark.read.csv(&quot;file.parquet&quot;)// hive tableval hive_table_df = spark.sql(&quot;select * from database_name.table_name&quot;) RDD to DF通过反射推断创建 DataFrame12345val rdd = sc.parallelize(Seq((&quot;Tom&quot;, 13),(&quot;Lily&quot;, 25)))import spark.implicits._val df = rdd.toDF(&quot;name&quot;,&quot;age&quot;) toDF() 方法定义如下： 1def toDF(colNames: String*): DataFrame 用于将强类型数据集合转换为具有重命名列的通用 DataFrame。在从 RDD 转换为具有有意义名称的 DataFrame 时非常方便。 通过 StructType 创建 DataFrame123456789101112131415import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}import org.apache.spark.sql.Rowval rdd = sc.parallelize(Seq((&quot;Tom&quot;, &quot;13&quot;),(&quot;Lily&quot;, &quot;25&quot;)))//创建 schemaval schema = StructType( List( StructField(&quot;name&quot;, StringType, false), StructField(&quot;age&quot;, IntegerType, false) ))//将 rdd 映射到 rdd[row] 上，并将数据格式化为相应的类型val rdd_row = rdd.map(x =&gt; Row(x._1,x._2.toInt))// 创建 dataframeval df = spark.createDataFrame(rdd_row, schema) 通过定义样例类创建 DataFrame123456789val rdd = sc.parallelize(Seq((&quot;Tom&quot;, &quot;13&quot;),(&quot;Lily&quot;, &quot;25&quot;)))//创建样例类case class User(name: String, age: Int)//将 rdd 映射到 rdd[User] 上val rdd_user = rdd.map(x =&gt; User(x._1,x._2.toInt))// 创建 dataframeval df = spark.createDataFrame(rdd_user)// 更简单一点，可以自动推断出 schema 创建 dataframeval df = rdd_user.toDF() RDD to DS通过定义样例类创建 Dataset123456789val rdd = sc.parallelize(Seq((&quot;Tom&quot;, &quot;13&quot;),(&quot;Lily&quot;, &quot;25&quot;)))//创建样例类case class User(name: String, age: Int)//将 rdd 映射到 rdd[User] 上val rdd_user = rdd.map(x =&gt; User(x._1,x._2.toInt))// 创建 dataframeval ds = spark.createDataset(rdd_user)// 更简单一点，可以自动推断出 schema 创建 datasetval ds = rdd_user.toDS() DS/DF to RDD123val df = spark.read.csv(&quot;file.csv&quot;)// 获取 rddval rdd = df.rdd DF to DS123//创建样例类case class User(name: String, age: Int)val ds = DataFrame.map(x=&gt; User(x.getAs(0), x.getAs(1))) DS to DF1val df = DataSet[DataTypeClass].toDF()","link":"/blog/2022/09/08/Spark%E4%B9%8BRDD%E3%80%81DF%E3%80%81DS%E5%88%9B%E5%BB%BA%E4%B8%8E%E8%BD%AC%E6%8D%A2/"},{"title":"Windows 下搭建 Spark","text":"版本选择Spark 部署模式分为本地单机（local）和集群模式，本地单机模式常用于本地开发程序与调试。集群模式又分为 Standalone 模式、Yarn 模式、Mesos 模式通过测试发现，以下版本组合报错信息最少 组件 版本 Spark 3.2.2 Hadoop 3.3.1 Scala 2.12.15 JDK 1.8 Spark 依赖库Spark 3.2.2 的依赖库版本如下： 依赖库 版本 Scala 2.12.15 Hadoop 3.3.1 安装步骤 下载安装 JDK，配置 JAVA_HOME 环境变量，将 JAVA_HOME/bin 添加至 Path 环境变量中。 下载安装 Scala，根据具体的操作系统，按照官网推荐的方式安装，无需配置 SCALA_HOME 环境变量。 下载安装 Hadoop，配置 HADOOP_HOME 环境变量，将 HADOOP_HOME/bin 添加至 Path 环境变量中。若在 Windows 上搭建，则还需要根据具体的 Hadoop 版本下载对应的 winutils.exe 和 hadoop.dll 文件，放入 HADOOP_HOME/bin 路径下，以获得在 Windows 上运行 Spark 的支持，避免以下报错信息：1Exception in thread “main” java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z 下载安装 Spark，配置 SPARK_HOME 环境变量，将 SPARK_HOME/bin 添加至 Path 环境变量中。进入 Spark 目录下的 conf 子目录下，根据需要修改 log4j.properties 等配置文件。log4j.properties 常见的配置如下：12345# 在终端输出 WARN 级别的日志，避免输出过多日志，影响查看log4j.rootCategory=WARN, console# 避免 ERROR ShutdownHookManager: Exception while deleting Spark temp dir 报错log4j.logger.org.apache.spark.util.ShutdownHookManager=OFFlog4j.logger.org.apache.spark.SparkEnv=ERROR 安装 PySpark如果上述安装步骤都已完成，就可以开始使用 Java 或 Scala 开发 Spark 程序了。对于 Python 用户，Spark 也提供了语言支持，只需要在 Spark 安装配置完成后，继续安装 PySpark 就可以使用 Python 开发 Spark 程序了： 使用 PyPI 安装1pip install pyspark=3.2.2 使用 Conda 安装1conda install -c conda-forge pyspark=3.2.2 注意： PySpark 的版本需要和 Spark 的版本保持一致，想要了解更多安装详情可以参考官方文档 报错解决 编码错误1UnicodeDecodeError: 'gbk' codec can't decode byte 0x82 in position 120: illegal multibyte sequence 通过添加以下环境变量解决：1PYTHONIOENCODING=utf8 任务运行时报错1org.apache.spark.SparkException: Python worker failed to connect back 通过添加以下环境变量解决：1PYSPARK_PYTHON=python","link":"/blog/2022/09/05/Windows%E4%B8%8B%E6%90%AD%E5%BB%BASpark/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/blog/2022/09/02/hello-world/"},{"title":"Spark 之自定义输出格式写入文件","text":"Spark 常用的保存文件方式 RDD 保存至文本文件1rdd.saveAsTextFile(&quot;path/result&quot;) RDD 以指定 Hadoop 输出格式保持至文件，仅支持 (key,value) 格式的 RDD1rdd.saveHadoopFile(&quot;path/result&quot;,classOf[T],classOf[T],classOf[outputFormat]) DataFrame 以指定格式保持至文件1df.write.mode(&quot;overwrite&quot;).option(&quot;header&quot;,&quot;true&quot;).format(&quot;csv&quot;).save(&quot;path/result&quot;) 以上都简单的，最普遍的保存文件的方式，但有时候是不能够满足我们的需求，使用上述的文件保存方式保存之后，文件名通常是 part-00000 的方式保存在输出文件夹中，并且还包含数据校验和文件 part-00000.crc 和 .SUCCESS 文件，其中 part-00000.crc 用来校验数据的完整性，.SUCCESS 文件用来表示本次输出任务成功完成。 自定义保存文件创建自定义 FileoutputFormat 类继承 MultipleTextOutputFormat 类并复写以下方法： 12345678910111213141516171819202122232425262728293031import org.apache.hadoop.fs.{FileSystem, Path}import org.apache.hadoop.mapred.{FileOutputFormat, JobConf}import org.apache.hadoop.mapred.lib.MultipleTextOutputFormatimport org.apache.hadoop.io.NullWritableclass CustomOutputFormat() extends MultipleTextOutputFormat[Any, Any] { override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = { //这里的key和value指的就是要写入文件的rdd对 key.asInstanceOf[String] + &quot;.csv&quot; } override def generateActualKey(key: Any, value: Any): String = { //输出文件中只保留value 故 key 返回为空 NullWritable.get() } override def checkOutputSpecs(ignored: FileSystem, job: JobConf): Unit = { val outDir: Path = FileOutputFormat.getOutputPath(job) if (outDir != null) { //相同文件名的文件自动覆盖 //避免第二次运行分区数少于第一次,历史数据覆盖失败,直接删除已经存在的目录 try { ignored.delete(outDir, true) } catch { case _: Throwable =&gt; {} } FileOutputFormat.setOutputPath(job, outDir) } }} 将 RDD 映射为 PairRDD1val pair_rdd = rdd.map(x=&gt;(x.split(&quot;,&quot;)(0),x)).partitionBy(new HashPartitioner(50)) 调用 saveAsHadoopFile 输出1pair_rdd.saveAsHadoopFile(output, classOf[String], classOf[String], classOf[CustomOutputFormat])","link":"/blog/2022/09/08/Spark%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E5%86%99%E5%85%A5%E6%96%87%E4%BB%B6/"},{"title":"本地仓库推送到 GitHub 仓库 main 分支","text":"GitHub 目前已经将用户创建的版本库的默认分支从 master 更改为了 main，详情见GitHub Change Blog。而 Git 默认创建的还是 master 分支，所以在本地仓库同步到 GitHub 时，会出现 error: failed to push some refs to 'github.com:imaginefish/blog.git' 报错，需要切换至 main 分区后再进行操作。 初始化版本库1git init 添加远程仓库1git remote add origin git@github.com:imaginefish/blog.git 拉取 GitHub 仓库到本地同步1git pull --rebase origin main 切换本地分支从 master 到 main1git checkout main 添加所有修改到暂存区1git add . 提交修改到版本库1git commit -m 'some messages' 推送到 GitHub 的 main 分支1git push origin main","link":"/blog/2022/09/05/%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E6%8E%A8%E9%80%81%E5%88%B0GitHub%E4%BB%93%E5%BA%93main%E5%88%86%E6%94%AF/"},{"title":"Hexo + Icarus + GitHub Pages 搭建个人博客","text":"先决条件需要先安装以下程序： Node.js (Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本) Git Node.js 版本限制强烈建议永远安装最新版本的 Hexo，以及推荐的 Node.js 版本。 Hexo 版本 最低兼容的 Node.js 版本 6.0+ 12.13.0 5.0+ 10.13.0 4.1 - 4.2 8.10 4.0 8.6 3.3 - 3.9 6.9 3.2 - 3.3 0.12 3.0 - 3.1 0.10 or iojs 0.0.1 - 2.8 0.10 安装 Hexo使用 npm 全局安装 Hexo。 1npm install -g hexo-cli 安装 icarus 指定路径初始化博客目录，并切换至该路径下，以blog路径为例12hexo init blogcd blog 使用 npm 安装 Hexo。1npm install hexo-theme-icarus 配置 Hexo 主题1hexo config theme icarus 创建 GitHub 仓库 创建 GitHub 仓库，并开启 Environments，配置 url 配置 ssh，确保可以 ssh 远程访问 GitHub，可以使用以下命令测试连接是否成功：1ssh -T git@github.com 如果出行以下信息，则说明连接成功：1Hi imaginefish! You've successfully authenticated, but GitHub does not provide shell access. 修改配置Hexo 配置Hexo 的配置文件在 blog 目录下，名为 _config.yml 修改语言为中文简体1language: zh-CN 修改时区1timezone: 'Asia/Shanghai' 修改博客网址，如果不配置会出现文件路径引用错误问题，导致 js、css、图片无法加载1url: https://imaginefish.github.io/blog 修改 hexo 部署方式，推送至 Github 仓库的 gh-pages 分支，实现博客部署1234deploy: type: git repository: git@github.com:imaginefish/blog.git branch: gh-pages Icarus 配置Icarus 的配置文件在 blog 目录下，名为 _config.icarus.yml 该主题导航栏无法跟随 Hexo 语言本地化，需要手动修改配置文件123456menu: 主页: / 归档: /archives 分类: /categories 标签: /tags 关于: /about 设置博主邮箱链接123Email: icon: fas fa-envelope url: mailto:imaginefishes@outlook.com 修改 sidebar 配置，使 toc 随文章下拉滚动123sidebar: left: sticky: true 用户访问量统计1busuanzi: true 布局配置文件布局配置文件遵循着与主题配置文件相同的格式和定义。 _config.post.yml 中的配置对所有文章生效，而 _config.page.yml 中的配置对所有自定义页面生效。 这两个文件将覆盖主题配置文件中的配置。例如，可以在 _config.post.yml 中把所有文章变为两栏布局：12345678910111213141516171819widgets: - type: toc position: left index: true collapsed: true depth: 3 - type: recent_posts position: left - type: categories position: left - type: tags position: left order_by: name amount: show_count: true 在 _config.icarus.yml 中把其他页面仍保持三栏布局：123456789101112131415- position: right type: recent_posts- position: right type: categories- position: left type: archives- position: right type: tags order_by: name amount: show_count: true 以下给出个人的完整 _config.icarus.yml 配置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143version: 5.1.0variant: defaultlogo: /img/logo.svghead: favicon: /img/favicon.svg manifest: name: short_name: start_url: theme_color: background_color: display: standalone icons: - src: '' sizes: '' type: structured_data: title: 梦鱼乡 description: 个人博客 url: https://github.com/imaginefish/blog author: 梦鱼 publisher: publisher_logo: image: meta: - '' rss: /atom.xmlnavbar: menu: 主页: / 归档: /archives 分类: /categories 标签: /tags 关于: /about links: GitHub: icon: fab fa-github url: https://github.com/imaginefish/blogfooter: links: Creative Commons: icon: fab fa-creative-commons url: https://creativecommons.org/ Attribution 4.0 International: icon: fab fa-creative-commons-by url: https://creativecommons.org/licenses/by/4.0/ Download on GitHub: icon: fab fa-github url: https://github.com/ppoffice/hexo-theme-icarusarticle: highlight: theme: atom-one-light clipboard: true fold: unfolded readtime: true update_time: true licenses: Creative Commons: icon: fab fa-creative-commons url: https://creativecommons.org/ Attribution: icon: fab fa-creative-commons-by url: https://creativecommons.org/licenses/by/4.0/ Noncommercial: icon: fab fa-creative-commons-nc url: https://creativecommons.org/licenses/by-nc/4.0/search: type: insight index_pages: truesidebar: left: sticky: true right: sticky: falsewidgets: - position: left type: profile author: 梦鱼 author_title: 大数据开发工程师 location: 中国.上海 avatar: /img/avatar.jpg avatar_rounded: true gravatar: follow_link: https://github.com/imaginefishes social_links: Github: icon: fab fa-github url: https://github.com/imaginefishes Email: icon: fas fa-envelope url: mailto:imaginefishes@outlook.com RSS: icon: fas fa-rss url: /atom.xml - position: left type: toc index: true collapsed: true depth: 3 - position: left type: links links: Hexo: https://hexo.io Icarus: https://ppoffice.github.io/hexo-theme-icarus - position: right type: recent_posts - position: right type: categories - position: left type: archives - position: right type: tags order_by: name amount: show_count: trueplugins: animejs: true back_to_top: true busuanzi: true cookie_consent: type: info theme: edgeless static: false position: bottom-left policyLink: https://www.cookiesandyou.com/ gallery: true katex: true mathjax: true outdated_browser: false progressbar: trueproviders: cdn: jsdelivr fontcdn: google iconcdn: fontawesome 其他问题 文章图片的引用路径若要插入本地图片，在博客根目录下找到 source 文件夹，在其下创建 img 子目录，将图片放置于此，通过 /img/xxx.jpg 路径引入。 文章图片如何居中显示该解决方案来源于 GitHub Issues。找到主题的安装路径，找到 article.styl 文件，我的路径为 node_modules\\hexo-theme-icarus\\include\\style\\article.styl，找到如下位置：1234567891011121314&amp;.article .article-meta, .article-tags color: $text-light .article-meta overflow-x: auto margin-bottom: .5rem .article-more @extend .button.is-light .content word-wrap: break-word font-size: $article-font-size 将 article 部分样式添加如下四行代码：12345678910111213141516171819&amp;.article .article-meta, .article-tags color: $text-light .article-meta overflow-x: auto margin-bottom: .5rem .article-more @extend .button.is-light .content word-wrap: break-word font-size: $article-font-size a img margin: auto display: block","link":"/blog/2022/09/02/hexo-icarus-github-pages-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","link":"/blog/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","link":"/blog/tags/HDFS/"},{"name":"Git","slug":"Git","link":"/blog/tags/Git/"},{"name":"Spark","slug":"Spark","link":"/blog/tags/Spark/"},{"name":"Hexo","slug":"Hexo","link":"/blog/tags/Hexo/"},{"name":"Icarus","slug":"Icarus","link":"/blog/tags/Icarus/"},{"name":"GitHub","slug":"GitHub","link":"/blog/tags/GitHub/"}],"categories":[{"name":"大数据","slug":"大数据","link":"/blog/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"版本控制","slug":"版本控制","link":"/blog/categories/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"},{"name":"个人博客","slug":"个人博客","link":"/blog/categories/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"}],"pages":[{"title":"关于我","text":"待更新…","link":"/blog/about/index.html"}]}